# ğŸš€ NeMo Guardrails + NVIDIA Triton + Ollama

A complete production-grade setup for running **NeMo Guardrails** inside **NVIDIA Triton Inference Server**, using **Ollama** as the backend LLM engine. This repository lets you run:

* âœ” Llama 3 / Mistral / any Ollama model inside Triton
* âœ” NeMo Guardrails (safety, policy, intent, conversation flows)
* âœ” A fully containerized deployment (Docker)
* âœ” Triton Python backend with custom `model.py`
* âœ” REST/gRPC inference via Triton API

This project is ideal for **production chatbots**, **enterprise safety layers**, and **secure inference pipelines**.

---

# ğŸ“¦ Features

### âœ… Triton-hosted Guardrails Engine

All safety logic runs inside Triton using the Python backend.

### âœ… Ollama LLM Integration

Uses Ollama locally to run models efficiently (GPU or CPU).

### âœ… Supports NeMo Guardrails v0.16.x

Fully tested with Colang 1.0 syntax and YAML config.

### âœ… Works with Any Chat LLM

Examples:

* llama3:8b-instruct
* mistral:7b
* phi3
* custom GGUF models

### âœ… Low VRAM Friendly

Optimized for laptops with RTX 4050 / 4â€“6 GB VRAM.

---

# ğŸ“ Repository Structure

```
model_repository/
â”‚
â””â”€â”€ guardrails/
    â”œâ”€â”€ 1/
    â”‚   â”œâ”€â”€ model.py
    â”‚   â””â”€â”€ rails/
    â”‚       â”œâ”€â”€ config.yml
    â”‚       â””â”€â”€ rails.co
    â”‚
    â””â”€â”€ config.pbtxt

Dockerfile
README.md  â† YOU ARE HERE
scripts/
â””â”€â”€ test_guardrails.py
```

---

# ğŸ³ 1. Build the Docker Image

```bash
docker build -t nemo-guardrails-triton .
```

Make sure Ollama is installed inside the image using:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

The image automatically pulls your model on first run.

---

# ğŸ–¥ï¸ 2. Start Triton + Ollama

Run the container:

```bash
docker run --gpus all -it --rm \
  -p 8000:8000 -p 8001:8001 -p 11434:11434 \
  -v "$(pwd)/model_repository:/models" \
  nemo-guardrails-triton
```

If the LLM model is not downloaded, Ollama will automatically fetch it.

---

# ğŸ“˜ 3. Guardrails Configuration

### `config.yml`

Defines the LLM backend, temperatures, intents, and bot policies.

Example:

```yaml
models:
  main:
    type: main
    engine: ollama
    model: llama3:8b-instruct-q5_K_M
    parameters:
      base_url: "http://127.0.0.1:11434"
      temperature: 0.3

rails:
  input:
    user:
      - intent: user_input
  output:
    bot:
      - type: bot_response

colang_files:
  - rails.co
```

---

### `rails.co`

You can define intents, safety, and flows:

```
define user_input
  user says anything
end

define bot_response
  bot says something
end
```

This is Colang v1.0 syntax.

---

# ğŸ§  4. Triton Model (`model.py`)

This Python backend file integrates Triton â†” NeMo Guardrails â†” Ollama.

> This version is compatible with NeMo Guardrails **0.16.x**.

Highlights:

* Loads Guardrails config
* Forwards user messages to LLM
* Returns bot message
* Handles string + dict outputs safely

The full working file is included in `model.py`.

---

# ğŸ§ª 5. Testing the Guardrails Engine

Use the included test script:

```bash
python3 scripts/test_guardrails.py
```

You should see:

```
===============================
  Testing LLM + Guardrails
===============================

User: Hello!
Bot: Hi! How can I assist you today?
```

---

# ğŸ”§ Troubleshooting

### â— Error: model not found (404)

Run:

```bash
ollama pull llama3:8b-instruct-q5_K_M
```

---

### â— KeyError: 'messages'

Means Guardrails v0.16 returned a **string**.
Use the latest `model.py` from this repo.

---

### â— AttributeError: 'str' has no attribute 'get'

Same issue â€” incorrect parsing.
Your `model.py` must normalize strings.

---

### â— GPU low VRAM mode

Ollama automatically enters low VRAM mode on 4â€“6GB GPUs.

This is normal.

---

# âœ¨ Performance Tips

* Use quantized models: `q4_K_M`, `q5_K_M`.
* Use `num_gpu=20` for RTX 4050.
* Reduce context length to 4096.
* Use smaller batch sizes.

---

# ğŸ›¡ Security

NeMo Guardrails provides:

* Content moderation
* Intent detection
* Custom safety rules
* Policy enforcement
* Conversation flow control

Suitable for enterprise deployments.

---

# ğŸ¤ Contributing

Pull requests are welcome! Feel free to open:

* Issues
* Bug reports
* Feature requests

---

# ğŸ“„ License

This project is licensed under the MIT License.

---

# â­ Support

If this helped you, please star â­ the repository!

# Output
![Model Output](assets/nemo_up.png)

![Model Output](assets/test_nemo.png)

